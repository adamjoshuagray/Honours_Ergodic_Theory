\documentclass{unswmaths}
\usepackage{unswshortcuts}
\usepackage{hyperref}
\begin{document}
\author{Adam J. Gray}
\title{Assignment 2}
\subject{Ergodic Theory}
\studentno{3329798}

\unswtitle

\section{}
\subsection{}
\begin{align*}
    T(\theta, \nu) = (\theta + \nu \mod 2\pi, \alpha\nu + \gamma \cos(\theta + \nu))
\end{align*}
\subsection{}
\begin{align*}
    J[T](\theta, \nu) = \left( 
    \begin{array}{cc}
        1   & 1 \\
        -\gamma \sin(\theta + \nu) & \alpha - \gamma\sin(\theta + \nu)
    \end{array}
    \right)
\end{align*}
and so
\begin{align*}
|J[T](\theta, \nu)| &= \alpha - \gamma\sin(\theta + \nu) + \gamma\sin(\theta + \nu) \\
    &= \alpha.
\end{align*}
So $ |J[T](\theta, \nu)| = 0 $ iff $ \alpha = 0 $. So since we have chosen $ \alpha > 0 $ this map is invertible everywhere by the inverse function theorem.
\subsection{}
\begin{figure}[h]
    \includegraphics[scale=0.5]{Question_3}
    \caption{Phase plot of velocity vs table phase with $ \theta_0 = 0.5, \nu_0 = 0.5 $}
\end{figure}
\subsection{}
The maximum post-bounce velocities occur near $ \theta = 0 $ ($\theta = 2\pi$). This makes sense because the piston is traveling in the opposite direction to the ball. This is happening fastest at $ \theta = 0 $ and so this results in the largest bounces. The minimum is found at $ \theta = \pi $. This also makes sense because this is when the piston is traveling in the same direction as the ball. This is happening fastest at $ \theta = \pi $ and this results in the largest slowdown in the bounce.

\subsection{}
\subsection{}
\label{qn_6_ergodic}
\begin{figure}[h]
    \includegraphics[scale=0.5]{Question_6_Ergodic}
    \caption{A bundle of 1000 points starting near $ \theta_0 = 0.5,  \nu_0 = 0.5 $ were iterated through $  T $ 10000 times}
\end{figure}

What figure 2 shows is that a small ball of points \emph{spreads out} in the phase space. Note that this ensemble of points roughly matches the orbit of one point. This would lead us to suspect that this process is Ergodic. 

\clearpage
\begin{figure}[h]
    \includegraphics[scale=0.5]{Question_6_Mixing}
    \label{qn_6_mixing}
    \caption{Two bundles of 200 points after iterating through  $ T $ 10000 times. Blue started near $ \theta_0 = 0.5, \nu_0 = 0.5 $ while red started near $ \theta_0 = 1, \nu_0 = 1 $ }
\end{figure}


Figure 3 suggests that the long term behaviour of the  points is independent of their starting point. That is two sets of balls mix together. Although only testing with two bundles of balls is hardly rigerous, even empirically, this would suggest that this process is mixing.
\subsection{}
Since we suspect that this process is Ergodic (and mixing) this question can be answered by watching the long term behavior of 1 point. We do this for a point starting at $ \theta_0 = 0.5, \nu_0 = 0.5 $ and see that $ 49.95 \% $ of the first 10000 bounces occur when $ \theta < \pi / 2 $ or $ \theta > 3\pi / 2 $, that is when the piston is moving up. We conclude that there is equal chance of the ball hitting on a down stroke compared to an upstroke.

\subsection{}
Again since we suspect that this process is Ergodic we just study the evolution of one point.
We see that with $ \theta_0 = 0.5, \nu_0 = 0.5 $ the maximum $ \nu $ acheived is $ 34.821 $ and the 945 bounces are within $ 1\%$ of this velocity over $ 10^6 $ iterations. This suggests that ball is within $ 1\%$ of its maximum post bounce speed about $ 0.945\% $ of the time.

\section{}
\subsection{}
See code in \emph{PageRank.m} available at \url{https://github.com/adamjoshuagray/Honours_Ergodic_Theory/tree/master/Assignment_2/Project_2_Code}

\subsection{}
See code in \emph{Question\_2.m} avaiable at \url{https://github.com/adamjoshuagray/Honours_Ergodic_Theory/tree/master/Assignment_2/Project_2_Code}. 

The resultant vector is
\begin{align*}    q =( 0.011785,   0.016793,   0.021465,   0.021465,   0.016961,   0.025212,   0.012925,   0.016961,\\
   0.024609,   0.027216,   0.049475,   0.012925,   0.013788,   0.025212,   0.025212,   0.020995,\\
   0.012925,   0.012925, 0.012925,   0.014928,   0.102503,   0.011785,   0.096000,   0.110181,   0.282829)
\end{align*}
\subsection{}
See code in \emph{Question\_3.m} avaiable at \url{https://github.com/adamjoshuagray/Honours_Ergodic_Theory/tree/master/Assignment_2/Project_2_Code}. 

For $ p = 0.2 $ the resultant vector is
\begin{align*}
    q = (   0.018888,
   0.020777,
   0.019809,
   0.019809,
   0.020140,
   0.020622,
   0.019136,
   0.020140,\\
   0.020812,
   0.021378,
   0.026139,
   0.019136,
   0.019643,
   0.020622,
   0.020622,
   0.021614,\\
   0.019136,
   0.019136,
   0.019136,
   0.019891,
   0.041430,
   0.018888,
   0.038194,
   0.041595,
   0.453310
).
\end{align*}
For $ p = 0.5 $ the resultant vector is
\begin{align*}
    q = (   0.015794,
   0.019742,
   0.019707,
   0.019707,
   0.018845,
   0.021730,
   0.016410,
   0.018845,\\
   0.021902,
   0.023310,
   0.035192,
   0.016410,
   0.017373,
   0.021730,
   0.021730,
   0.021910,\\
   0.016410,
   0.016410,
   0.016410,
   0.017989,
   0.070430,
   0.015794,
   0.064402,
   0.072763,
   0.379055
).
\end{align*}
\begin{figure}[h]
    \includegraphics[scale=0.5]{Graph}
    \caption{The layout of the network}
\end{figure}
In figure 4 we see that there are basically 3 important nodes, 21, 23 and 24. When choosing a smaller value for $ p $ less weight is given to 21 and 23 and all of it goes to 24. As we might want to keep 21 and 23 as important nodes we would select a higher value of $ p $. Say $ p = 0.85 $.
To see a comparison of different weightings given for different values of $ p $ see figure 5.

\begin{figure}[h]
    \includegraphics[scale=0.7]{Different_ps}
    \caption{Different weightings for different values of $ p $.}
\end{figure}
\clearpage
\subsection{}
See code in \emph{Question\_4.m} avaiable at \url{https://github.com/adamjoshuagray/Honours_Ergodic_Theory/tree/master/Assignment_2/Project_2_Code}. 

The solution converges exponentially with the error at step n given by
\begin{align*}
    Error(n) = \exp(-0.50655n - 0.22944).
\end{align*}
This is true for about $ n \leq 65 $, after that the error has reached machine precision and cannot improve any further.
See figure 6 for a plot of $ \log(Error(n)) $.
\begin{figure}[h]
    \includegraphics[scale=0.5]{Log_Error_n}
    \caption{Exponential convergence to the stationary vector. Note that at about $n = 65 $ the graph flattens out. This is because the error has reached machine precision.}
\end{figure}

\end{document}




